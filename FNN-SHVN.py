# -*- coding: utf-8 -*-
"""svhn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKmjoC3wMepSor_gz_FOG-5HmbS3AHUR
"""

#CNN
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

#Load SVHN Dataset
dataset, info = tfds.load("svhn_cropped", split=["train", "test"], as_supervised=True, with_info=True)
train_dataset, test_dataset = dataset

def dataset_to_numpy(dataset):
    images, labels = [], []
    for img, label in tfds.as_numpy(dataset):
        images.append(img)
        labels.append(label)
    return np.array(images), np.array(labels)

X_train, y_train = dataset_to_numpy(train_dataset)
X_test, y_test = dataset_to_numpy(test_dataset)

# Normalize the Data (Using Mean & Std)
mean = np.mean(X_train, axis=(0,1,2))
std = np.std(X_train, axis=(0,1,2))

X_train = (X_train - mean) / (std + 1e-7)
X_test = (X_test - mean) / (std + 1e-7)

#  One-Hot Encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

#  Data Augmentation
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1
)
datagen.fit(X_train)

#  Build CNN Model
model = Sequential([
    Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(32, 32, 3)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(128, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(256, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),  # Helps prevent overfitting
    Dense(10, activation='softmax')  # 10 output classes
])

#  Compile Model with Adam Optimizer
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#  split data into training and validation sets
X_train_split, X_value, y_train_split, y_value = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# training data
train_generator = datagen.flow(X_train_split, y_train_split, batch_size=64)

# Train Model using train_generator and validation_data
history = model.fit(train_generator,
                    epochs=27,
                    validation_data=(X_value, y_value),  # Pass validation set separately
                    verbose=1)

# Evaluate Model on Test Data
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)
print(f'Test Accuracy: {test_acc:.4f}')

# Plot Training History
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.title('Model Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.title('Model Loss')

plt.show()

# Visualize Some Predictions (Fixed Clipping Issue)
predictions = model.predict(X_test)

# Denormalize images before displaying
X_test_denorm = (X_test * std) + mean  # Reverse normalization

plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(np.clip(X_test_denorm[i] / 255.0, 0, 1))  # Ensure values are in [0,1]
    plt.xlabel(f"True: {np.argmax(y_test[i])}, Pred: {np.argmax(predictions[i])}")
plt.show()

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Load Data
def load_data():
    dataset = tfds.load("svhn_cropped", split=["train", "test"], as_supervised=True)
    train_data, test_data = dataset[0], dataset[1]
    X_train, y_train = dataset_to_numpy(train_data)
    X_test, y_test = dataset_to_numpy(test_data)
    return X_train, y_train, X_test, y_test

# Convert dataset to numpy arrays
def dataset_to_numpy(dataset):
    images, labels = [], []
    for img, label in tfds.as_numpy(dataset):
        images.append(img)
        labels.append(label)
    return np.array(images) / 255.0, np.array(labels).squeeze()


# Build Model
def build_model(input_shape):
    model = Sequential([
        Flatten(input_shape=input_shape),
        Dense(256, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0005)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0005)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0005)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(10, activation='softmax')
    ])
    return model

# Train Model
def train_and_evaluate(model, X_train, y_train, X_value, y_value, batch_size, use_augmentation):
    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    if use_augmentation:
        datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1)
        datagen.fit(X_train)
        history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),
                            epochs=30, validation_data=(X_value, y_value), verbose=1)
    else:
        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=30, validation_data=(X_value, y_value), verbose=1)

    return history

# Plot Accuracy and Loss
def plot_metrics(history, title):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title(f'{title} - Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title(f'{title} - Loss')

    plt.show()

# Evaluate Model
def evaluate_model(model, X_test, y_test, title):
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print(f'{title} - Test Accuracy: {test_acc:.4f}')

    y_pred = np.argmax(model.predict(X_test), axis=1)
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.show()

# Main Function
def main():
    X_train, y_train, X_test, y_test = load_data()
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

    model = build_model(input_shape=(32, 32, 3))

    history = train_and_evaluate(model, X_train, y_train, X_val, y_val, batch_size=32, use_augmentation=True)
    plot_metrics(history, "With Data Augmentation")
    evaluate_model(model, X_test, y_test, "With Data Augmentation")

if __name__ == "__main__":
    main()

